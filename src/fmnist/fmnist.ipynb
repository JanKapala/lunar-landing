{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "laden-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "freelance-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "apart-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(net_output, labels):\n",
    "    predicted = net_output.argmax(dim=1)\n",
    "    correct = (predicted == labels).sum()\n",
    "    examples = len(labels)\n",
    "    return (correct / examples).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "iraqi-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train_tensor = torch.tensor(deepcopy(x_train), dtype=torch.float32)\n",
    "x_train_tensor /= 255.\n",
    "x_test_tensor = torch.tensor(deepcopy(x_test), dtype=torch.float32)\n",
    "x_test_tensor /= 255.\n",
    "y_train_tensor = torch.tensor(deepcopy(y_train), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(deepcopy(y_test), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "reflected-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(TensorDataset(x_train_tensor, y_train_tensor), shuffle=False, batch_size=32)\n",
    "test_set = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=len(x_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "comprehensive-leeds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28]), torch.Size([10000]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_set))[0].shape, next(iter(test_set))[1].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "sufficient-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0/100 -- train_loss: 2.2838, test_loss: 2.2196, train_acc: 0.2726,  test_acc: 0.3457\n",
      "EPOCH 1/100 -- train_loss: 2.0394, test_loss: 1.8939, train_acc: 0.4864,  test_acc: 0.6451\n",
      "EPOCH 2/100 -- train_loss: 1.8346, test_loss: 1.8099, train_acc: 0.6693,  test_acc: 0.6743\n",
      "EPOCH 3/100 -- train_loss: 1.7853, test_loss: 1.7729, train_acc: 0.6921,  test_acc: 0.7129\n",
      "EPOCH 4/100 -- train_loss: 1.7471, test_loss: 1.7360, train_acc: 0.7457,  test_acc: 0.7603\n",
      "EPOCH 5/100 -- train_loss: 1.7160, test_loss: 1.7120, train_acc: 0.7724,  test_acc: 0.7719\n",
      "EPOCH 6/100 -- train_loss: 1.6979, test_loss: 1.6982, train_acc: 0.7815,  test_acc: 0.7791\n",
      "EPOCH 7/100 -- train_loss: 1.6871, test_loss: 1.6896, train_acc: 0.7876,  test_acc: 0.7842\n",
      "EPOCH 8/100 -- train_loss: 1.6800, test_loss: 1.6836, train_acc: 0.7922,  test_acc: 0.7878\n",
      "EPOCH 9/100 -- train_loss: 1.6748, test_loss: 1.6792, train_acc: 0.7961,  test_acc: 0.7906\n",
      "EPOCH 10/100 -- train_loss: 1.6708, test_loss: 1.6759, train_acc: 0.7988,  test_acc: 0.7932\n",
      "EPOCH 11/100 -- train_loss: 1.6674, test_loss: 1.6731, train_acc: 0.8018,  test_acc: 0.7945\n",
      "EPOCH 12/100 -- train_loss: 1.6646, test_loss: 1.6707, train_acc: 0.8042,  test_acc: 0.7964\n",
      "EPOCH 13/100 -- train_loss: 1.6622, test_loss: 1.6687, train_acc: 0.8064,  test_acc: 0.7986\n",
      "EPOCH 14/100 -- train_loss: 1.6600, test_loss: 1.6669, train_acc: 0.8082,  test_acc: 0.7995\n",
      "EPOCH 15/100 -- train_loss: 1.6581, test_loss: 1.6654, train_acc: 0.8102,  test_acc: 0.8005\n",
      "EPOCH 16/100 -- train_loss: 1.6563, test_loss: 1.6641, train_acc: 0.8117,  test_acc: 0.8013\n",
      "EPOCH 17/100 -- train_loss: 1.6548, test_loss: 1.6629, train_acc: 0.8133,  test_acc: 0.8028\n",
      "EPOCH 18/100 -- train_loss: 1.6534, test_loss: 1.6618, train_acc: 0.8145,  test_acc: 0.8040\n",
      "EPOCH 19/100 -- train_loss: 1.6521, test_loss: 1.6608, train_acc: 0.8153,  test_acc: 0.8044\n",
      "EPOCH 20/100 -- train_loss: 1.6509, test_loss: 1.6599, train_acc: 0.8164,  test_acc: 0.8049\n",
      "EPOCH 21/100 -- train_loss: 1.6498, test_loss: 1.6591, train_acc: 0.8174,  test_acc: 0.8049\n",
      "EPOCH 22/100 -- train_loss: 1.6488, test_loss: 1.6584, train_acc: 0.8182,  test_acc: 0.8052\n",
      "EPOCH 23/100 -- train_loss: 1.6479, test_loss: 1.6577, train_acc: 0.8189,  test_acc: 0.8059\n",
      "EPOCH 24/100 -- train_loss: 1.6470, test_loss: 1.6571, train_acc: 0.8197,  test_acc: 0.8064\n",
      "EPOCH 25/100 -- train_loss: 1.6462, test_loss: 1.6565, train_acc: 0.8203,  test_acc: 0.8075\n",
      "EPOCH 26/100 -- train_loss: 1.6454, test_loss: 1.6559, train_acc: 0.8211,  test_acc: 0.8078\n",
      "EPOCH 27/100 -- train_loss: 1.6447, test_loss: 1.6554, train_acc: 0.8218,  test_acc: 0.8084\n",
      "EPOCH 28/100 -- train_loss: 1.6440, test_loss: 1.6549, train_acc: 0.8224,  test_acc: 0.8088\n",
      "EPOCH 29/100 -- train_loss: 1.6433, test_loss: 1.6545, train_acc: 0.8230,  test_acc: 0.8094\n",
      "EPOCH 30/100 -- train_loss: 1.6427, test_loss: 1.6540, train_acc: 0.8233,  test_acc: 0.8100\n",
      "EPOCH 31/100 -- train_loss: 1.6421, test_loss: 1.6536, train_acc: 0.8237,  test_acc: 0.8095\n",
      "EPOCH 32/100 -- train_loss: 1.6415, test_loss: 1.6532, train_acc: 0.8240,  test_acc: 0.8096\n",
      "EPOCH 33/100 -- train_loss: 1.6409, test_loss: 1.6529, train_acc: 0.8245,  test_acc: 0.8103\n",
      "EPOCH 34/100 -- train_loss: 1.6404, test_loss: 1.6525, train_acc: 0.8249,  test_acc: 0.8106\n",
      "EPOCH 35/100 -- train_loss: 1.6399, test_loss: 1.6521, train_acc: 0.8253,  test_acc: 0.8109\n",
      "EPOCH 36/100 -- train_loss: 1.6394, test_loss: 1.6518, train_acc: 0.8259,  test_acc: 0.8114\n",
      "EPOCH 37/100 -- train_loss: 1.6389, test_loss: 1.6514, train_acc: 0.8265,  test_acc: 0.8118\n",
      "EPOCH 38/100 -- train_loss: 1.6384, test_loss: 1.6511, train_acc: 0.8268,  test_acc: 0.8125\n",
      "EPOCH 39/100 -- train_loss: 1.6379, test_loss: 1.6507, train_acc: 0.8274,  test_acc: 0.8130\n",
      "EPOCH 40/100 -- train_loss: 1.6375, test_loss: 1.6504, train_acc: 0.8277,  test_acc: 0.8132\n",
      "EPOCH 41/100 -- train_loss: 1.6370, test_loss: 1.6500, train_acc: 0.8282,  test_acc: 0.8137\n",
      "EPOCH 42/100 -- train_loss: 1.6366, test_loss: 1.6497, train_acc: 0.8287,  test_acc: 0.8138\n",
      "EPOCH 43/100 -- train_loss: 1.6362, test_loss: 1.6494, train_acc: 0.8293,  test_acc: 0.8140\n",
      "EPOCH 44/100 -- train_loss: 1.6357, test_loss: 1.6492, train_acc: 0.8298,  test_acc: 0.8140\n",
      "EPOCH 45/100 -- train_loss: 1.6353, test_loss: 1.6489, train_acc: 0.8300,  test_acc: 0.8142\n",
      "EPOCH 46/100 -- train_loss: 1.6349, test_loss: 1.6486, train_acc: 0.8305,  test_acc: 0.8145\n",
      "EPOCH 47/100 -- train_loss: 1.6345, test_loss: 1.6484, train_acc: 0.8308,  test_acc: 0.8144\n",
      "EPOCH 48/100 -- train_loss: 1.6342, test_loss: 1.6481, train_acc: 0.8310,  test_acc: 0.8145\n",
      "EPOCH 49/100 -- train_loss: 1.6338, test_loss: 1.6479, train_acc: 0.8315,  test_acc: 0.8151\n",
      "EPOCH 50/100 -- train_loss: 1.6334, test_loss: 1.6477, train_acc: 0.8319,  test_acc: 0.8152\n",
      "EPOCH 51/100 -- train_loss: 1.6331, test_loss: 1.6474, train_acc: 0.8323,  test_acc: 0.8153\n",
      "EPOCH 52/100 -- train_loss: 1.6327, test_loss: 1.6472, train_acc: 0.8325,  test_acc: 0.8153\n",
      "EPOCH 53/100 -- train_loss: 1.6324, test_loss: 1.6470, train_acc: 0.8328,  test_acc: 0.8157\n",
      "EPOCH 54/100 -- train_loss: 1.6321, test_loss: 1.6469, train_acc: 0.8332,  test_acc: 0.8158\n",
      "EPOCH 55/100 -- train_loss: 1.6317, test_loss: 1.6467, train_acc: 0.8336,  test_acc: 0.8161\n",
      "EPOCH 56/100 -- train_loss: 1.6314, test_loss: 1.6465, train_acc: 0.8340,  test_acc: 0.8161\n",
      "EPOCH 57/100 -- train_loss: 1.6311, test_loss: 1.6464, train_acc: 0.8341,  test_acc: 0.8160\n",
      "EPOCH 58/100 -- train_loss: 1.6308, test_loss: 1.6462, train_acc: 0.8344,  test_acc: 0.8164\n",
      "EPOCH 59/100 -- train_loss: 1.6305, test_loss: 1.6460, train_acc: 0.8347,  test_acc: 0.8163\n",
      "EPOCH 60/100 -- train_loss: 1.6302, test_loss: 1.6459, train_acc: 0.8349,  test_acc: 0.8163\n",
      "EPOCH 61/100 -- train_loss: 1.6298, test_loss: 1.6457, train_acc: 0.8353,  test_acc: 0.8163\n",
      "EPOCH 62/100 -- train_loss: 1.6295, test_loss: 1.6455, train_acc: 0.8354,  test_acc: 0.8164\n",
      "EPOCH 63/100 -- train_loss: 1.6292, test_loss: 1.6454, train_acc: 0.8358,  test_acc: 0.8168\n",
      "EPOCH 64/100 -- train_loss: 1.6289, test_loss: 1.6452, train_acc: 0.8361,  test_acc: 0.8170\n",
      "EPOCH 65/100 -- train_loss: 1.6286, test_loss: 1.6451, train_acc: 0.8363,  test_acc: 0.8172\n",
      "EPOCH 66/100 -- train_loss: 1.6283, test_loss: 1.6448, train_acc: 0.8365,  test_acc: 0.8173\n",
      "EPOCH 67/100 -- train_loss: 1.6280, test_loss: 1.6445, train_acc: 0.8370,  test_acc: 0.8172\n",
      "EPOCH 68/100 -- train_loss: 1.6277, test_loss: 1.6444, train_acc: 0.8373,  test_acc: 0.8175\n",
      "EPOCH 69/100 -- train_loss: 1.6274, test_loss: 1.6442, train_acc: 0.8375,  test_acc: 0.8171\n",
      "EPOCH 70/100 -- train_loss: 1.6271, test_loss: 1.6441, train_acc: 0.8378,  test_acc: 0.8173\n",
      "EPOCH 71/100 -- train_loss: 1.6268, test_loss: 1.6439, train_acc: 0.8381,  test_acc: 0.8175\n",
      "EPOCH 72/100 -- train_loss: 1.6266, test_loss: 1.6438, train_acc: 0.8384,  test_acc: 0.8177\n",
      "EPOCH 73/100 -- train_loss: 1.6263, test_loss: 1.6436, train_acc: 0.8386,  test_acc: 0.8176\n",
      "EPOCH 74/100 -- train_loss: 1.6260, test_loss: 1.6435, train_acc: 0.8389,  test_acc: 0.8179\n",
      "EPOCH 75/100 -- train_loss: 1.6257, test_loss: 1.6434, train_acc: 0.8393,  test_acc: 0.8182\n",
      "EPOCH 76/100 -- train_loss: 1.6254, test_loss: 1.6433, train_acc: 0.8396,  test_acc: 0.8183\n",
      "EPOCH 77/100 -- train_loss: 1.6252, test_loss: 1.6432, train_acc: 0.8397,  test_acc: 0.8184\n",
      "EPOCH 78/100 -- train_loss: 1.6249, test_loss: 1.6430, train_acc: 0.8400,  test_acc: 0.8187\n",
      "EPOCH 79/100 -- train_loss: 1.6246, test_loss: 1.6429, train_acc: 0.8402,  test_acc: 0.8193\n",
      "EPOCH 80/100 -- train_loss: 1.6244, test_loss: 1.6428, train_acc: 0.8404,  test_acc: 0.8193\n",
      "EPOCH 81/100 -- train_loss: 1.6242, test_loss: 1.6427, train_acc: 0.8407,  test_acc: 0.8194\n",
      "EPOCH 82/100 -- train_loss: 1.6239, test_loss: 1.6430, train_acc: 0.8409,  test_acc: 0.8192\n",
      "EPOCH 83/100 -- train_loss: 1.6237, test_loss: 1.6429, train_acc: 0.8411,  test_acc: 0.8187\n",
      "EPOCH 84/100 -- train_loss: 1.6235, test_loss: 1.6428, train_acc: 0.8412,  test_acc: 0.8190\n",
      "EPOCH 85/100 -- train_loss: 1.6233, test_loss: 1.6427, train_acc: 0.8415,  test_acc: 0.8193\n",
      "EPOCH 86/100 -- train_loss: 1.6231, test_loss: 1.6428, train_acc: 0.8417,  test_acc: 0.8189\n",
      "EPOCH 87/100 -- train_loss: 1.6228, test_loss: 1.6426, train_acc: 0.8420,  test_acc: 0.8191\n",
      "EPOCH 88/100 -- train_loss: 1.6226, test_loss: 1.6425, train_acc: 0.8422,  test_acc: 0.8185\n",
      "EPOCH 89/100 -- train_loss: 1.6224, test_loss: 1.6423, train_acc: 0.8425,  test_acc: 0.8188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 90/100 -- train_loss: 1.6222, test_loss: 1.6422, train_acc: 0.8427,  test_acc: 0.8191\n",
      "EPOCH 91/100 -- train_loss: 1.6220, test_loss: 1.6422, train_acc: 0.8428,  test_acc: 0.8191\n",
      "EPOCH 92/100 -- train_loss: 1.6218, test_loss: 1.6421, train_acc: 0.8430,  test_acc: 0.8195\n",
      "EPOCH 93/100 -- train_loss: 1.6215, test_loss: 1.6420, train_acc: 0.8432,  test_acc: 0.8194\n",
      "EPOCH 94/100 -- train_loss: 1.6213, test_loss: 1.6418, train_acc: 0.8435,  test_acc: 0.8196\n",
      "EPOCH 95/100 -- train_loss: 1.6211, test_loss: 1.6417, train_acc: 0.8437,  test_acc: 0.8196\n",
      "EPOCH 96/100 -- train_loss: 1.6209, test_loss: 1.6417, train_acc: 0.8439,  test_acc: 0.8197\n",
      "EPOCH 97/100 -- train_loss: 1.6206, test_loss: 1.6416, train_acc: 0.8442,  test_acc: 0.8199\n",
      "EPOCH 98/100 -- train_loss: 1.6204, test_loss: 1.6416, train_acc: 0.8444,  test_acc: 0.8195\n",
      "EPOCH 99/100 -- train_loss: 1.6202, test_loss: 1.6416, train_acc: 0.8446,  test_acc: 0.8192\n"
     ]
    }
   ],
   "source": [
    "net = FMnistNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    accum_loss = []\n",
    "    accum_acc = []\n",
    "    \n",
    "    for data, labels in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        net_output = net(data)\n",
    "        loss = loss_fn(net_output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accum_loss.append(loss.item())\n",
    "        accum_acc.append(acc(net_output, labels))\n",
    "        \n",
    "    train_avg_loss = np.mean(accum_loss)\n",
    "    train_avg_acc = np.mean(accum_acc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_data, test_labels = next(iter(test_set))\n",
    "        test_output = net(test_data)\n",
    "        test_loss = loss_fn(test_output, test_labels)\n",
    "        test_acc = acc(test_output, test_labels)\n",
    "        \n",
    "        \n",
    "    print(f\"EPOCH {epoch+1}/{epochs} -- train_loss: {train_avg_loss:.4f}, test_loss: {test_loss:.4f}, train_acc: {train_avg_acc:.4f},  test_acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-investing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
